# -*- coding: utf-8 -*-
"""LSTM_Models_And_Attack.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/misha345a/E-commerce_Reviews_Classifier/blob/main/LSTM_Models_And_Attack.ipynb

## Goal
Train a deep learning model to predict Recommended vs Not Recommended classification based on customer reviews. <br> Then, attack the trained models using TextAttack to evaluate model robustness.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture 
# 
# !pip install openpyxl -U

"""## Dataset"""

import pandas as pd
from tqdm.notebook import tqdm_notebook

# initiate tqdm for pandas.apply() functions
tqdm_notebook.pandas()

# expand notebook display options for dataframes
pd.set_option('display.max_colwidth', 200)
pd.options.display.max_columns = 999
pd.options.display.max_rows = 300

# load the dataset of your choice
# dataset = pd.read_excel('/content/Raw_Dataset_(Cleaned).xlsx')
dataset = pd.read_excel('/content/Upsampled_Dataset.xlsx')
# dataset = pd.read_excel('/content/Augmented_Dataset.xlsx')

# check value counts of prediction class
dataset['Recommended IND'].value_counts()

# define a random seed for reproducible results
random_state = 42

# resize dataset 
dataset = dataset[0:1000]

# shuffle the dataset rows
dataset = dataset.sample(frac=1,
                         random_state=random_state)

# check value counts of prediction class
dataset['Recommended IND'].value_counts()

from sklearn.model_selection import train_test_split

X = dataset.drop('Recommended IND', axis=1)
y = dataset['Recommended IND']

# split the dataset into an 80% training and 20% test set
X_train, X_test, y_train, y_test = train_test_split(X, y, 
                                                    test_size=0.2, 
                                                    random_state=random_state,
                                                    shuffle=True)

"""## Tokenization"""

import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Dense, LSTM
from sklearn.metrics import classification_report

# utilize the most frequently apprearing words in the corpus
num_words = 10000

# tokenize the training data
tokenizer = Tokenizer(num_words=num_words,
                      filters='!"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n1234567890') 
corpus = X_train['Review Text'].tolist() + X_test['Review Text'].tolist()
tokenizer.fit_on_texts(corpus)

# define the data word index
word_index = tokenizer.word_index
# print(word_index)

# encode training/test data into sequences
X_train_seq = tokenizer.texts_to_sequences(X_train['Review Text'].tolist())
X_test_seq = tokenizer.texts_to_sequences(X_test['Review Text'].tolist())

# define the max number of words to consider in each review
maxlen = max([len(x) for x in X_train_seq])
print(f"Max sequence length: {maxlen}\n")

# truncate and pad the training/test input sequences
X_train_pad = pad_sequences(X_train_seq, maxlen=maxlen)
X_test_pad = pad_sequences(X_test_seq, maxlen=maxlen)

# output the resulting dimensions 
print("Padded shape (training):".ljust(25), X_train_pad.shape)
print("Padded shape (test):".ljust(25), X_test_pad.shape)

"""## LSTM Neural Network"""

# initiate LSTM for sequence classification
model = Sequential()

# embed each numeric in a 50-dimensional vector
model.add(Embedding(len(word_index) + 1,
                    50,     
                    input_length=maxlen))

# add bidirectional LSTM layer
model.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3))

# add a classifier 
model.add(Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])
    
model.summary()

batch_size = 512
num_epochs = 5

# train the model
model.fit(X_train_pad, y_train, 
          epochs=num_epochs,
          batch_size=batch_size)

"""## Evaluation"""

# evaluate model on the test set
model.evaluate(X_test_pad, y_test)
y_test_pred = (model.predict(X_test_pad) >= 0.5).astype("int32")

print(classification_report(y_test, y_test_pred))

"""## Save Model (optional)"""

# save the entire model
model.save('LSTM_Raw_Dataset_old')

"""## Load Model (optional)"""

# reload a fresh Keras model from the saved model
new_model = tf.keras.models.load_model('/content/LSTM_Raw_Dataset_old')

# retrieve the maxlen variable of the model
model_config = new_model.get_config()
maxlen = model_config['layers'][0]['config']['batch_input_shape'][1]
print(maxlen)

"""## Custom Predictions (Examples)"""

# 5 randomely selected reviews
reviews = ["this dress is perfection! so pretty and flattering.",
           "this is my new favorite top! looks and fits as described.",
           "i could wear this every day, it is stylish and comfortable",
           "material is too thin and quality is poor",
           "it is nice material but the design makes you look like a pregnant lady"]

def model_pred(text):
  """
  Use the trained LSTM to make predictions on new examples.
  """
  tokens = tokenizer.texts_to_sequences([text]) 
  tokens_pad = pad_sequences(tokens, maxlen=maxlen)
  tokens_pad.shape
  model_pred = model.predict(tokens_pad)

  conf_val = model_pred[0][0]
  if conf_val>=0.5:
    print( f"'{text}'\nRecommended | {int(conf_val*100)}% Confidence\n")
  else:
    print( f"'{text}'\nNot Recommended | {int(conf_val*100)}% Confidence\n")

for i in reviews:
  model_pred(i)

"""## Textual Adversarial Attack"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# 
# !pip install textattack
# !pip install tensorflow_text

from textattack.models.wrappers import ModelWrapper
from textattack.datasets import Dataset
import numpy as np
import torch
import nltk
nltk.download('omw-1.4')

class CustomTensorFlowModelWrapper(ModelWrapper):
  """
  Implementation of a model wrapper class to
  run TextAttack with a custom TensorFlow model.
  """
  def __init__(self, model):
    self.model = model

  def __call__(self, text_input_list):
    # retrieve model prediction
    text_array = np.array(text_input_list)
    tokens = tokenizer.texts_to_sequences(text_input_list) 
    tokens_pad = pad_sequences(tokens, maxlen=maxlen)
    model_pred = self.model.predict(tokens_pad)

    # return prediction scores as torch.Tensors
    logits = torch.FloatTensor(model_pred)
    logits = logits.squeeze(dim=-1)

    # for each output, index 0 corresponds to the negative 
    # and index 1 corresponds to the positive confidence 
    final_preds = torch.stack((1-logits, logits), dim=1)

    return final_preds

# example output
CustomTensorFlowModelWrapper(model)(["this is negative text. bad terrible awful.",
                                     "this is positive text. great amazing love"])

# example of a successful text atack which fools the model into predicting the wrong label
t1 = 'i love the tie dye and the accent stitching. back detail is fun!'
t2 = 'i adore the tie colouring and the accent stitching. back detail is amusing!'
CustomTensorFlowModelWrapper(model)([t1,t2])

"""## Creating the Attack"""

# initialize the model wrapper with the trained LSTM
model_wrapper = CustomTensorFlowModelWrapper(model)

# textattack requires custom datasets to be presented as a list of (input, ground-truth label) pairs
data_pairs = []
for input, label in zip(dataset['Review Text'], dataset['Recommended IND']):
  data_pairs.append((input, label))

new_dataset = Dataset(data_pairs, shuffle=True)

# construct the four fundamental components of the attack
from textattack.goal_functions.classification import UntargetedClassification
from textattack.constraints.pre_transformation import RepeatModification, StopwordModification
from textattack.constraints.semantics import WordEmbeddingDistance
from textattack.transformations import WordSwapEmbedding
from textattack.search_methods import GreedyWordSwapWIR
from textattack import Attack

goal_function = UntargetedClassification(model_wrapper)

constraints = [
    RepeatModification(),
    StopwordModification(),
    WordEmbeddingDistance(min_cos_sim=0.9)
]

transformation = WordSwapEmbedding(max_candidates=50)

search_method = GreedyWordSwapWIR(wir_method="delete")

# construct the actual attack
attack = Attack(goal_function, constraints, transformation, search_method)

from textattack import Attacker
from textattack import AttackArgs

# attack until 1000 successfull attacks are reached
attack_args = AttackArgs(num_examples=10, 
                         random_seed=random_state)

attacker = Attacker(attack, new_dataset, attack_args)

attack_results = attacker.attack_dataset()

# # display the attack results and the differences
# logger = CSVLogger(color_method='html')

# for result in attack_results:
#     logger.log_attack_result(result)

# from IPython.core.display import display, HTML
# display(HTML(logger.df[['original_text', 'perturbed_text']].to_html(escape=False)))

